{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyboardDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = self.pad_data(data)\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "    def pad_data(self, data):\n",
    "        new_data_list = []\n",
    "        average_len = np.mean([len(sublist) for sublist in data])\n",
    "        for sublist in data:\n",
    "            curr_len = 0\n",
    "            new_data = sublist\n",
    "            while (curr_len < average_len):\n",
    "                if curr_len > len(sublist):\n",
    "                    new_data.append([-1.0, -1, -1])\n",
    "                else: \n",
    "                    new_data.append(sublist[curr_len])\n",
    "                curr_len += 1\n",
    "            new_data_list.append(new_data)\n",
    "                \n",
    "        return new_data_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_size, num_classes, num_layers=1, num_heads=2, hidden_size=64, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        # self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        \n",
    "        # Create an instance of nn.TransformerEncoderLayer\n",
    "        encoder_layer = nn.TransformerEncoderLayer(embedding_size, num_heads, hidden_size, dropout)\n",
    "        \n",
    "        # Pass the encoder layer instance to nn.TransformerEncoder\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        \n",
    "        self.fc = nn.Linear(embedding_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # embedded = self.embedding(x)\n",
    "        # embedded = embedded.permute(1, 0, 2)  # Change dimensions for transformer\n",
    "        # output = self.transformer(embedded)\n",
    "        output = self.transformer(x)\n",
    "        output = output.mean(dim=0)  # Average across time steps\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, trg in iterator:\n",
    "            output = model(src, trg)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12000 5-second intervals, 3000 intervals from each person\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidanmelvin/Documents/college/sem6/cmsc472/final-project/venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# labels key:\n",
    "# 0: Aidan\n",
    "# 1: Srujan\n",
    "# 2: Eric\n",
    "# 3: Tony\n",
    "\n",
    "vocab_size = 113  # Assuming ASCII characters\n",
    "embedding_size = 3\n",
    "num_classes = 4\n",
    "num_layers = 2\n",
    "num_heads = 1\n",
    "hidden_size = 128\n",
    "dropout = 0.1\n",
    "learning_rate = 0.001\n",
    "# batch_size = 32\n",
    "batch_size = 1\n",
    "epochs = 10\n",
    "file_prefix = '../'\n",
    "datapoints_per_person = 3000\n",
    "\n",
    "fh = open(f'{file_prefix}aidan_final_data_overlapping.json', 'r')\n",
    "aidan_data = json.load(fh)[:datapoints_per_person]\n",
    "\n",
    "fh = open(f'{file_prefix}srujan_final_data_overlapping.json', 'r')\n",
    "srujan_data = json.load(fh)[:datapoints_per_person]\n",
    "\n",
    "fh = open(f'{file_prefix}eric_final_data_overlapping.json', 'r')\n",
    "eric_data = json.load(fh)[:datapoints_per_person]\n",
    "\n",
    "fh = open(f'{file_prefix}tony_final_data_overlapping.json', 'r')\n",
    "tony_data = json.load(fh)[:datapoints_per_person]\n",
    "\n",
    "data = aidan_data + srujan_data + eric_data + tony_data\n",
    "labels = ([0] * datapoints_per_person) + ([1] * datapoints_per_person) + ([2] * datapoints_per_person) + ([3] * datapoints_per_person)\n",
    "\n",
    "print(f'There are {len(data)} 5-second intervals, {datapoints_per_person} intervals from each person')\n",
    "\n",
    "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = KeyboardDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = KeyboardDataset(val_data, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "model = TransformerModel(vocab_size, embedding_size, num_classes, num_layers, num_heads, hidden_size, dropout)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4l/4b0lplcx5nq62s05h9x8z5p80000gn/T/ipykernel_25982/1702590594.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape:  torch.Size([90, 3])\n",
      "x.shape:  torch.Size([54, 3])\n",
      "x.shape:  torch.Size([45, 3])\n",
      "x.shape:  torch.Size([41, 3])\n",
      "x.shape:  torch.Size([53, 3])\n",
      "x.shape:  torch.Size([91, 3])\n",
      "x.shape:  torch.Size([136, 3])\n",
      "x.shape:  torch.Size([87, 3])\n",
      "x.shape:  torch.Size([99, 3])\n",
      "x.shape:  torch.Size([100, 3])\n",
      "x.shape:  torch.Size([87, 3])\n",
      "x.shape:  torch.Size([60, 3])\n",
      "x.shape:  torch.Size([72, 3])\n",
      "x.shape:  torch.Size([82, 3])\n",
      "x.shape:  torch.Size([66, 3])\n",
      "x.shape:  torch.Size([84, 3])\n",
      "x.shape:  torch.Size([114, 3])\n",
      "x.shape:  torch.Size([112, 3])\n",
      "x.shape:  torch.Size([51, 3])\n",
      "x.shape:  torch.Size([54, 3])\n",
      "x.shape:  torch.Size([45, 3])\n",
      "x.shape:  torch.Size([99, 3])\n",
      "x.shape:  torch.Size([48, 3])\n",
      "x.shape:  torch.Size([71, 3])\n",
      "x.shape:  torch.Size([52, 3])\n",
      "x.shape:  torch.Size([64, 3])\n",
      "x.shape:  torch.Size([104, 3])\n",
      "x.shape:  torch.Size([92, 3])\n",
      "x.shape:  torch.Size([74, 3])\n",
      "x.shape:  torch.Size([111, 3])\n",
      "x.shape:  torch.Size([48, 3])\n",
      "x.shape:  torch.Size([122, 3])\n",
      "x.shape:  torch.Size([49, 3])\n",
      "x.shape:  torch.Size([93, 3])\n",
      "x.shape:  torch.Size([55, 3])\n",
      "x.shape:  torch.Size([107, 3])\n",
      "x.shape:  torch.Size([69, 3])\n",
      "x.shape:  torch.Size([58, 3])\n",
      "x.shape:  torch.Size([46, 3])\n",
      "x.shape:  torch.Size([103, 3])\n",
      "x.shape:  torch.Size([60, 3])\n",
      "x.shape:  torch.Size([104, 3])\n",
      "x.shape:  torch.Size([57, 3])\n",
      "x.shape:  torch.Size([74, 3])\n",
      "x.shape:  torch.Size([87, 3])\n",
      "x.shape:  torch.Size([43, 3])\n",
      "x.shape:  torch.Size([80, 3])\n",
      "x.shape:  torch.Size([66, 3])\n",
      "x.shape:  torch.Size([56, 3])\n",
      "x.shape:  torch.Size([60, 3])\n",
      "x.shape:  torch.Size([103, 3])\n",
      "x.shape:  torch.Size([52, 3])\n",
      "x.shape:  torch.Size([103, 3])\n",
      "x.shape:  torch.Size([84, 3])\n",
      "x.shape:  torch.Size([99, 3])\n",
      "x.shape:  torch.Size([52, 3])\n",
      "x.shape:  torch.Size([53, 3])\n",
      "x.shape:  torch.Size([58, 3])\n",
      "x.shape:  torch.Size([41, 3])\n",
      "x.shape:  torch.Size([117, 3])\n",
      "x.shape:  torch.Size([44, 3])\n",
      "x.shape:  torch.Size([60, 3])\n",
      "x.shape:  torch.Size([66, 3])\n",
      "x.shape:  torch.Size([48, 3])\n",
      "x.shape:  torch.Size([51, 3])\n",
      "x.shape:  torch.Size([87, 3])\n",
      "x.shape:  torch.Size([104, 3])\n",
      "x.shape:  torch.Size([48, 3])\n",
      "x.shape:  torch.Size([42, 3])\n",
      "x.shape:  torch.Size([45, 3])\n",
      "x.shape:  torch.Size([43, 3])\n",
      "x.shape:  torch.Size([48, 3])\n",
      "x.shape:  torch.Size([48, 3])\n",
      "x.shape:  torch.Size([68, 3])\n",
      "x.shape:  torch.Size([65, 3])\n",
      "x.shape:  torch.Size([97, 3])\n",
      "x.shape:  torch.Size([73, 3])\n",
      "x.shape:  torch.Size([63, 3])\n",
      "x.shape:  torch.Size([47, 3])\n",
      "x.shape:  torch.Size([91, 3])\n",
      "x.shape:  torch.Size([42, 3])\n",
      "x.shape:  torch.Size([76, 3])\n",
      "x.shape:  torch.Size([80, 3])\n",
      "x.shape:  torch.Size([50, 3])\n",
      "x.shape:  torch.Size([90, 3])\n",
      "x.shape:  torch.Size([82, 3])\n",
      "x.shape:  torch.Size([92, 3])\n",
      "x.shape:  torch.Size([93, 3])\n",
      "x.shape:  torch.Size([61, 3])\n",
      "x.shape:  torch.Size([69, 3])\n",
      "x.shape:  torch.Size([119, 3])\n",
      "x.shape:  torch.Size([98, 3])\n",
      "x.shape:  torch.Size([79, 3])\n",
      "x.shape:  torch.Size([68, 3])\n",
      "x.shape:  torch.Size([56, 3])\n",
      "x.shape:  torch.Size([71, 3])\n",
      "x.shape:  torch.Size([75, 3])\n",
      "x.shape:  torch.Size([95, 3])\n",
      "x.shape:  torch.Size([45, 3])\n",
      "x.shape:  torch.Size([95, 3])\n",
      "x.shape:  torch.Size([47, 3])\n",
      "x.shape:  torch.Size([53, 3])\n",
      "x.shape:  torch.Size([92, 3])\n",
      "x.shape:  torch.Size([74, 3])\n",
      "x.shape:  torch.Size([83, 3])\n",
      "x.shape:  torch.Size([79, 3])\n",
      "x.shape:  torch.Size([100, 3])\n",
      "x.shape:  torch.Size([65, 3])\n",
      "x.shape:  torch.Size([98, 3])\n",
      "x.shape:  torch.Size([44, 3])\n",
      "x.shape:  torch.Size([53, 3])\n",
      "x.shape:  torch.Size([132, 3])\n",
      "x.shape:  torch.Size([60, 3])\n",
      "x.shape:  torch.Size([77, 3])\n",
      "x.shape:  torch.Size([44, 3])\n",
      "x.shape:  torch.Size([96, 3])\n",
      "x.shape:  torch.Size([84, 3])\n",
      "x.shape:  torch.Size([84, 3])\n",
      "x.shape:  torch.Size([112, 3])\n",
      "x.shape:  torch.Size([42, 3])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mto(device), labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m     33\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m---> 34\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     37\u001b[0m _, predicted_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/college/sem6/cmsc472/final-project/venv/lib/python3.11/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/college/sem6/cmsc472/final-project/venv/lib/python3.11/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/college/sem6/cmsc472/final-project/venv/lib/python3.11/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    correct_train = 0  \n",
    "    total_train = 0   \n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        labels = torch.tensor(labels)\n",
    "        \n",
    "        # re-format the data\n",
    "        # batch length list, where each list is length 96\n",
    "        reformatted_data = [[] for i in range(batch_size)]\n",
    "        for group in inputs:\n",
    "            j = 0\n",
    "            times = group[0]  # len batch_size\n",
    "            characters = group[1]  # len batch_size\n",
    "            updown = group[2]  # len batch_size\n",
    "            while j < batch_size:\n",
    "                reformatted_data[j].append([float(times[j]), float(characters[j]), float(updown[j])])\n",
    "                j = j + 1\n",
    "        \n",
    "        reformatted_data = torch.tensor(reformatted_data)[0]\n",
    "\n",
    "        outputs = model(reformatted_data.to(device))\n",
    "        # print('outputs.shape: ', outputs.shape)\n",
    "        loss = criterion(outputs.to(device), labels[0].to(device))\n",
    "        train_losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "       \n",
    "        _, predicted_train = torch.max(outputs.data, dim = 0)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted_train == labels[0].to(device)).sum().item()\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Training Accuracy: {train_accuracy:.4f}')\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_predictions = []\n",
    "    val_targets = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            labels = torch.tensor(labels)\n",
    "            reformatted_data = [[] for i in range(batch_size)]\n",
    "            for group in inputs:\n",
    "                j = 0\n",
    "                times = group[0]  # len batch_size\n",
    "                characters = group[1]  # len batch_size\n",
    "                updown = group[2]  # len batch_size\n",
    "                while j < batch_size:\n",
    "                    reformatted_data[j].append([float(times[j]), float(characters[j]), float(updown[j])])\n",
    "                    j = j + 1\n",
    "            \n",
    "            reformatted_data = torch.tensor(reformatted_data)[0]\n",
    "            # print(reformatted_data.shape)\n",
    "\n",
    "            outputs = model(reformatted_data.to(device))\n",
    "            loss = criterion(outputs.to(device), labels[0].to(device))\n",
    "            val_losses.append(loss)\n",
    "          \n",
    "            predicted = torch.argmax(outputs)\n",
    "            val_predictions.append(predicted.cpu().numpy())\n",
    "            val_targets.append(labels.cpu().numpy())\n",
    "    \n",
    "    # print(f'val targets (shape {len(val_targets)}): {val_targets}')\n",
    "    # print(f'val predictions (shape {len(val_predictions)}): {val_predictions}')\n",
    "    val_accuracy = accuracy_score(val_targets, val_predictions)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f'Epoch [{epoch+1}/{epochs}], Validation Accuracy: {val_accuracy:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
